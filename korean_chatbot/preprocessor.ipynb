{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:34:40.090166Z",
     "start_time": "2020-11-18T14:34:38.328005Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "CHANGE_FILTER = re.compile(FILTERS) # 미리 Complie\n",
    "PAD, PAD_INDEX = \"<PAD>\", 0 # 패딩 토큰\n",
    "STD, STD_INDEX = \"<SOS>\", 1 # 시작 토큰\n",
    "END, END_INDEX = \"<END>\", 2 # 종료 토큰\n",
    "UNK, UNK_INDEX = \"<UNK>\", 3 # 사전에 없음\n",
    "MARKER = [PAD,STD,END,UNK]\n",
    "MAX_SEQUNECE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:34:40.116031Z",
     "start_time": "2020-11-18T14:34:40.108663Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path,header=0)\n",
    "    question, answer = list(df['Q']),list(df['A'])\n",
    "    \n",
    "    return question, answer\n",
    "\n",
    "def tokenizing_data(data):\n",
    "    words = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word) \n",
    "\n",
    "    return [word for word in words if word]\n",
    "\n",
    "def analzing_morhology(data):\n",
    "    okt = Okt()\n",
    "    results = []\n",
    "    \n",
    "    for seq in tqdm(data):\n",
    "        result = \" \".join(okt.morphs(seq.replace(' ', '')))\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def make_vocab(vocab_list):\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:36:50.570268Z",
     "start_time": "2020-11-18T14:36:50.556589Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_vocab_dict(raw_data_path, vocab_path):\n",
    "    vocab_list = []\n",
    "    \n",
    "    if not os.path.exists(vocab_path):\n",
    "        question, answer = load_data(raw_data_path)\n",
    "        \n",
    "        data = []\n",
    "        data.extend(question)\n",
    "        data.extend(answer)\n",
    "\n",
    "        # Tokenizing \n",
    "        words = tokenizing_data(data)\n",
    "        words = list(set(words))\n",
    "        words[:0] = MARKER # 사전에 정의한 토큰을 단어 리스트 앞에 추가\n",
    "\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as vocab_f:\n",
    "            for word in words:\n",
    "                vocab_f.write(word + '\\n')\n",
    "    \n",
    "    with open(vocab_path, 'r', encoding='utf-8') as vocab_f:\n",
    "        for line in vocab_f:\n",
    "            vocab_list.append(line.strip())\n",
    "            \n",
    "    word2idx, idx2word = make_vocab(vocab_list)\n",
    "    \n",
    "    return word2idx, idx2word, len(word2idx)\n",
    "\n",
    "def processing_encoder_input(value, dictionary):\n",
    "    sequences_input_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    for sequence in value :\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        \n",
    "        for word in sequence.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        \n",
    "        # truncating      \n",
    "        if len(sequence_index) > MAX_SEQUNECE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUNECE]\n",
    "        \n",
    "        # padding\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (MAX_SEQUNECE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_input_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_input_index), sequences_length\n",
    "\n",
    "def processing_decoder_input(value, dictionary):\n",
    "    sequences_output_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER,\"\",sequence)\n",
    "        sequence_index = []\n",
    "\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "\n",
    "        if len(sequence_index) > MAX_SEQUNECE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUNECE]\n",
    "\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
    "        sequences_output_index.append(sequence_index)\n",
    "        \n",
    "    return np.asarray(sequences_output_index), sequences_length\n",
    "\n",
    "def processing_decoder_target(value, dictionary):\n",
    "    sequences_target_index = []\n",
    "    \n",
    "    for sequence in value :\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        \n",
    "        if len(sequence_index) >= MAX_SEQUNECE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUNECE-1] + [dictionary[END]]\n",
    "        else :\n",
    "            sequence_index += [dictionary[END]]\n",
    "\n",
    "        sequence_index += (MAX_SEQUNECE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_target_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T14:38:43.943652Z",
     "start_time": "2020-11-18T14:38:43.624140Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    RAW_DATA_PATH = './ChatBotData.csv'\n",
    "    VOCAB_PATH = './vocab.txt'\n",
    "    \n",
    "    inputs, outputs = load_data(RAW_DATA_PATH)\n",
    "    char2idx, idx2char, vocab_size = load_vocab_dict(RAW_DATA_PATH, VOCAB_PATH)\n",
    "\n",
    "    # encoder/decoder input /target\n",
    "    index_inputs, input_seq_len = processing_encoder_input(inputs, char2idx)\n",
    "    index_outputs, output_seq_len = processing_decoder_input(outputs, char2idx)\n",
    "    index_targets = processing_decoder_target(outputs, char2idx)\n",
    "\n",
    "    data_configs = {}\n",
    "    data_configs['char2idx'] =char2idx\n",
    "    data_configs['idx2char'] = idx2char\n",
    "    data_configs['vocab_size'] = vocab_size\n",
    "    data_configs['pad_symbol'] = PAD\n",
    "    data_configs['std_symbol'] = STD\n",
    "    data_configs['end_symbol'] = END\n",
    "    data_configs['unk_symbol'] = UNK\n",
    "\n",
    "    DATA_IN_PATH = './data_in/'\n",
    "    np.save('train_inputs.npy', index_inputs)\n",
    "    np.save('train_outputs.npy', index_outputs)\n",
    "    np.save('train_targets.npy', index_targets)\n",
    "\n",
    "    json.dump(data_configs, open('data_configs.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
