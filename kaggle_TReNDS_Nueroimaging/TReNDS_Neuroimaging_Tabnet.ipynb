{"cells":[{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install pytorch-tabnet","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting pytorch-tabnet\n  Downloading pytorch_tabnet-1.1.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.45.0)\nRequirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.22.2.post1)\nRequirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.5.0)\nRequirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.4.1)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.18.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (0.14.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\nInstalling collected packages: pytorch-tabnet\nSuccessfully installed pytorch-tabnet-1.1.0\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Preliminaries\nimport numpy as np\nimport pandas as pd \nimport os\nimport random\n\n#Visuals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Torch and Tabnet\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n#Sklearn only for splitting\nfrom sklearn.model_selection import KFold","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 10  # you can specify your folds here\nseed      = 2000   # seed for reproducible results","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seed Everything\n\nSeeding Everything for Reproducible Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(seed)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metric\n\nSince Tabnet allows us to create a MULTIREGRESSOR , we don't have to create multiple models and loop through them . I have modified the metric to account for that"},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    \n    overall_score = 0\n    \n    weights = [.3, .175, .175, .175, .175]\n#     weights = [.2, .2, .2, .2, .2]\n    \n    for i,w in zip(range(y_true.shape[1]),weights):\n        ind_score = np.mean(np.sum(np.abs(y_true[:,i] - y_pred[:,i]), axis=0)/np.sum(y_true[:,i], axis=0))\n        overall_score += w*ind_score\n    \n    return overall_score","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\n\nMostly Taken from Ahmet's kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc_df = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\nloading_df = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nfeatures = fnc_features + loading_features\n\nlabels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\ntarget_features = list(labels_df.columns[1:])\nlabels_df[\"is_train\"] = True\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"((5877, 1411), (5877, 1411))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating FOLDS\n\ndf = df.dropna().reset_index(drop=True)\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1,random_state=seed).reset_index(drop=True)\n\nkf = KFold(n_splits=NUM_FOLDS)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=df)):\n    df.loc[val_, 'kfold'] = fold","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1/500\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = TabNetRegressor(n_d=16,\n                       n_a=16,\n                       n_steps=4,\n                       gamma=1.3,\n                       n_independent=2,\n                       n_shared=2,\n                       seed=seed,\n                       optimizer_fn = torch.optim.Adam,\n                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)","execution_count":10,"outputs":[{"output_type":"stream","text":"Device used : cuda\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Engine"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.zeros((test_df.shape[0],len(target_features), NUM_FOLDS))  #A 3D TENSOR FOR STORING RESULTS OF ALL FOLDS","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(fold):\n    df_train = df[df.kfold != fold]\n    df_valid = df[df.kfold == fold]\n    \n    X_train = df_train[features].values\n    Y_train = df_train[target_features].values\n    \n    X_valid = df_valid[features].values\n    Y_valid = df_valid[target_features].values\n    \n    y_oof = np.zeros((df_valid.shape[0],len(target_features)))   # Out of folds validation\n    \n    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n     \n    model.fit(X_train = X_train,\n             y_train = Y_train,\n             X_valid = X_valid,\n             y_valid = Y_valid,\n             max_epochs = 1000,\n             patience =70)\n              \n    \n    print(\"--------Validating For fold {}------------\".format(fold+1))\n    \n    y_oof = model.predict(X_valid)\n    y_test[:,:,fold] = model.predict(test_df[features].values)\n    \n    val_score = metric(Y_valid,y_oof)\n    \n    print(\"Validation score: {:<8.5f}\".format(val_score))\n    \n    # VISUALIZTION\n    plt.figure(figsize=(12,6))\n    plt.plot(model.history['train']['loss'])\n    plt.plot(model.history['valid']['loss'])\n    \n    #Plotting Metric\n    #plt.plot([-x for x in model.history['train']['metric']])\n    #plt.plot([-x for x in model.history['valid']['metric']])","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I am hiding the output of training please unhide the output to look at the results and Loss plots for any fold"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=0)","execution_count":null,"outputs":[{"output_type":"stream","text":"--------Training Begining for fold 1-------------\nWill train until validation stopping metric hasn't improved in 70 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\nCurrent learning rate:  0.02\n| 1     | -2818.64697 |  -2809.07544 |   1.9       \nCurrent learning rate:  0.02\n| 2     | -2595.14380 |  -2701.57910 |   2.5       \nCurrent learning rate:  0.02\n| 3     | -2341.99902 |  -2505.89111 |   3.2       \nCurrent learning rate:  0.02\n| 4     | -2006.01172 |  -2247.20728 |   3.9       \nCurrent learning rate:  0.02\n| 5     | -1568.41577 |  -1833.99927 |   4.5       \nCurrent learning rate:  0.02\n| 6     | -1048.53760 |  -1130.55151 |   5.1       \nCurrent learning rate:  0.02\n| 7     | -546.77753 |  -512.48138 |   5.7       \nCurrent learning rate:  0.02\n| 8     | -257.98816 |  -187.60574 |   6.3       \nCurrent learning rate:  0.02\n| 9     | -219.85384 |  -150.50302 |   6.9       \nCurrent learning rate:  0.02\n| 10    | -167.94089 |  -262.67984 |   7.4       \nCurrent learning rate:  0.02\n| 11    | -155.50165 |  -141.99042 |   8.0       \nCurrent learning rate:  0.02\n| 12    | -146.02080 |  -182.53622 |   8.7       \nCurrent learning rate:  0.02\n| 13    | -140.33267 |  -139.41125 |   9.4       \nCurrent learning rate:  0.02\n| 14    | -139.79984 |  -149.43587 |   9.9       \nCurrent learning rate:  0.02\n| 15    | -138.19276 |  -140.12070 |   10.5      \nCurrent learning rate:  0.02\n| 16    | -138.10825 |  -138.67899 |   11.0      \nCurrent learning rate:  0.02\n| 17    | -137.74815 |  -150.30307 |   11.6      \nCurrent learning rate:  0.02\n| 18    | -136.94034 |  -140.36591 |   12.1      \nCurrent learning rate:  0.02\n| 19    | -137.38391 |  -137.96651 |   12.7      \nCurrent learning rate:  0.02\n| 20    | -137.28629 |  -138.24614 |   13.2      \nCurrent learning rate:  0.02\n| 21    | -136.08223 |  -136.17966 |   13.8      \nCurrent learning rate:  0.02\n| 22    | -136.08127 |  -136.76099 |   14.3      \nCurrent learning rate:  0.02\n| 23    | -135.99510 |  -135.87715 |   14.9      \nCurrent learning rate:  0.02\n| 24    | -134.66367 |  -136.22104 |   15.4      \nCurrent learning rate:  0.02\n| 25    | -135.11893 |  -137.55496 |   16.0      \nCurrent learning rate:  0.02\n| 26    | -134.42545 |  -135.85155 |   16.5      \nCurrent learning rate:  0.02\n| 27    | -134.73845 |  -136.54829 |   17.1      \nCurrent learning rate:  0.02\n| 28    | -135.23251 |  -137.64197 |   17.6      \nCurrent learning rate:  0.02\n| 29    | -134.27319 |  -137.42572 |   18.2      \nCurrent learning rate:  0.02\n| 30    | -133.40279 |  -138.22876 |   18.7      \nCurrent learning rate:  0.02\n| 31    | -133.78793 |  -141.79195 |   19.3      \nCurrent learning rate:  0.02\n| 32    | -133.34087 |  -142.68515 |   20.0      \nCurrent learning rate:  0.02\n| 33    | -133.97902 |  -136.37903 |   20.6      \nCurrent learning rate:  0.02\n| 34    | -133.64870 |  -134.97661 |   21.1      \nCurrent learning rate:  0.02\n| 35    | -133.90994 |  -138.10168 |   21.7      \nCurrent learning rate:  0.02\n| 36    | -133.28513 |  -136.57318 |   22.2      \nCurrent learning rate:  0.02\n| 37    | -131.51834 |  -136.81535 |   22.8      \nCurrent learning rate:  0.02\n| 38    | -131.19473 |  -143.18579 |   23.3      \nCurrent learning rate:  0.02\n| 39    | -130.72418 |  -142.40729 |   23.9      \nCurrent learning rate:  0.02\n| 40    | -129.49207 |  -137.56041 |   24.5      \nCurrent learning rate:  0.02\n| 41    | -128.03340 |  -136.16469 |   25.0      \nCurrent learning rate:  0.02\n| 42    | -126.30099 |  -138.10300 |   25.6      \nCurrent learning rate:  0.02\n| 43    | -125.34258 |  -141.26305 |   26.1      \nCurrent learning rate:  0.02\n| 44    | -124.07357 |  -140.26723 |   26.6      \nCurrent learning rate:  0.02\n| 45    | -123.01544 |  -140.61569 |   27.2      \nCurrent learning rate:  0.02\n| 46    | -122.18880 |  -138.56165 |   27.7      \nCurrent learning rate:  0.02\n| 47    | -121.74452 |  -135.77147 |   28.2      \nCurrent learning rate:  0.02\n| 48    | -121.20353 |  -135.04192 |   28.8      \nCurrent learning rate:  0.02\n| 49    | -121.01033 |  -138.46400 |   29.3      \nCurrent learning rate:  0.02\n| 50    | -120.05735 |  -135.90959 |   29.9      \nCurrent learning rate:  0.02\n| 51    | -119.71057 |  -138.93094 |   30.5      \nCurrent learning rate:  0.02\n| 52    | -119.37946 |  -133.54355 |   31.2      \nCurrent learning rate:  0.02\n| 53    | -119.55902 |  -136.79086 |   31.7      \nCurrent learning rate:  0.02\n| 54    | -119.69380 |  -134.55809 |   32.3      \nCurrent learning rate:  0.02\n| 55    | -118.87801 |  -135.41499 |   32.8      \nCurrent learning rate:  0.02\n| 56    | -118.27803 |  -136.68994 |   33.3      \nCurrent learning rate:  0.02\n| 57    | -118.07310 |  -135.43094 |   33.9      \nCurrent learning rate:  0.02\n| 58    | -118.38157 |  -135.11526 |   34.4      \nCurrent learning rate:  0.02\n| 59    | -119.25124 |  -133.43013 |   35.0      \nCurrent learning rate:  0.02\n| 60    | -118.92947 |  -131.52914 |   35.5      \nCurrent learning rate:  0.02\n| 61    | -119.38387 |  -135.10271 |   36.1      \nCurrent learning rate:  0.02\n| 62    | -118.76754 |  -135.15608 |   36.6      \nCurrent learning rate:  0.02\n| 63    | -118.27954 |  -136.03850 |   37.1      \nCurrent learning rate:  0.02\n| 64    | -117.53044 |  -136.21869 |   37.7      \nCurrent learning rate:  0.02\n| 65    | -116.98112 |  -135.98857 |   38.2      \nCurrent learning rate:  0.02\n| 66    | -117.39099 |  -135.89499 |   38.7      \nCurrent learning rate:  0.02\n| 67    | -116.36761 |  -132.90892 |   39.3      \nCurrent learning rate:  0.02\n| 68    | -116.30400 |  -135.17984 |   39.8      \nCurrent learning rate:  0.02\n| 69    | -115.46110 |  -134.19067 |   40.3      \nCurrent learning rate:  0.02\n| 70    | -115.44224 |  -133.62646 |   40.9      \nCurrent learning rate:  0.02\n| 71    | -115.50476 |  -128.60735 |   41.6      \nCurrent learning rate:  0.02\n| 72    | -115.39902 |  -129.66904 |   42.2      \nCurrent learning rate:  0.02\n| 73    | -114.68693 |  -134.68147 |   42.7      \nCurrent learning rate:  0.02\n| 74    | -114.65581 |  -140.08139 |   43.3      \nCurrent learning rate:  0.02\n| 75    | -114.59219 |  -140.84166 |   43.8      \nCurrent learning rate:  0.02\n| 76    | -114.42339 |  -135.84018 |   44.3      \nCurrent learning rate:  0.02\n| 77    | -114.10445 |  -130.43501 |   44.9      \nCurrent learning rate:  0.02\n| 78    | -113.86568 |  -128.22246 |   45.4      \nCurrent learning rate:  0.02\n| 79    | -113.43599 |  -129.85033 |   46.0      \nCurrent learning rate:  0.02\n| 80    | -113.21737 |  -134.46544 |   46.5      \nCurrent learning rate:  0.02\n| 81    | -113.06328 |  -135.19951 |   47.1      \nCurrent learning rate:  0.02\n| 82    | -113.20805 |  -136.37700 |   47.6      \nCurrent learning rate:  0.02\n| 83    | -113.25580 |  -135.76884 |   48.2      \nCurrent learning rate:  0.02\n| 84    | -113.08147 |  -141.57191 |   48.7      \nCurrent learning rate:  0.02\n| 85    | -113.01848 |  -139.34528 |   49.3      \nCurrent learning rate:  0.02\n| 86    | -112.53893 |  -139.69917 |   49.8      \nCurrent learning rate:  0.02\n| 87    | -111.89010 |  -137.71510 |   50.3      \nCurrent learning rate:  0.02\n| 88    | -112.43488 |  -136.10410 |   50.9      \nCurrent learning rate:  0.02\n| 89    | -112.06001 |  -135.96689 |   51.4      \nCurrent learning rate:  0.02\n| 90    | -111.73728 |  -135.07039 |   51.9      \nCurrent learning rate:  0.02\n| 91    | -111.94344 |  -133.47333 |   52.6      \nCurrent learning rate:  0.02\n| 92    | -111.19612 |  -133.75847 |   53.2      \nCurrent learning rate:  0.02\n| 93    | -111.17397 |  -133.53659 |   53.8      \nCurrent learning rate:  0.02\n| 94    | -112.46062 |  -137.40128 |   54.3      \nCurrent learning rate:  0.02\n| 95    | -111.79472 |  -133.22104 |   55.0      \nCurrent learning rate:  0.02\n| 96    | -111.30404 |  -136.07414 |   55.5      \nCurrent learning rate:  0.02\n| 97    | -110.65696 |  -136.76860 |   56.1      \nCurrent learning rate:  0.02\n| 98    | -110.62224 |  -137.44560 |   56.6      \nCurrent learning rate:  0.02\n| 99    | -112.14028 |  -137.28299 |   57.2      \nCurrent learning rate:  0.02\n| 100   | -110.88063 |  -135.72110 |   57.7      \nCurrent learning rate:  0.02\n| 101   | -110.26488 |  -136.75618 |   58.2      \n","name":"stdout"},{"output_type":"stream","text":"Current learning rate:  0.02\n| 102   | -110.61096 |  -136.56258 |   58.8      \nCurrent learning rate:  0.02\n| 103   | -110.64238 |  -135.03262 |   59.3      \n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = y_test.mean(axis=-1) # Taking mean of all the fold predictions\ntest_df[target_features] = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.melt(test_df, id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\nsub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}